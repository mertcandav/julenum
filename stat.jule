// Copyright 2025 mertcandav.
// Use of this source code is governed by a BSD 3-Clause
// license that can be found in the LICENSE file.

use "std/math"

// Computes the chi-square statistic between two vectors: observed and expected.
// The lengths of obs and exp must be equal, otherwise it panics.
//
// Mathematically, the chi-square statistic is defined as:
//	χ² = Σᵢ [ (Oᵢ - Eᵢ)² / Eᵢ ]
//	where:
//		Oᵢ = observed value at index i,
//		Eᵢ = expected value at index i,
//		and the summation is over all elements.
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
#disable boundary
fn ChiSquare[T: float](obs: []T, exp: []T): f64 {
	if len(obs) != len(exp) {
		panic("julenum: ChiSquare: different vector lengths")
	}
	mut r := 0.0
	mut i := 0
	for i < len(obs); i++ {
		x, y := f64(obs[i]), f64(exp[i])
		if x == 0 && y == 0 {
			continue
		}
		diff := x - y
		r += (diff * diff) / y
	}
	ret r
}

// Computes the chi-square distance between two vectors: x and y.
// The lengths of x and y must be equal, otherwise it panics.
//
// Mathematically, the chi-square distance is defined as:
//	χ² = 1/2 * Σᵢ [ (xᵢ - yᵢ)² / (xᵢ + yᵢ) ]
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
#disable boundary
fn ChiSquareDistance[T: float](x: []T, y: []T): f64 {
	if len(x) != len(y) {
		panic("julenum: ChiSquareDistance: different vector lengths")
	}
	let mut r: f64
	mut i := 0
	for i < len(x); i++ {
		xi, yi := f64(x[i]), f64(y[i])
		if xi == 0 && yi == 0 {
			continue
		}
		diff := xi - yi
		r += (diff * diff) / (xi + yi)
	}
	ret 0.5 * r
}

// Computes the mean of the data set x.
//
// Mathematically, the mean is defined as:
//	Mean = Σᵢ [ xᵢ ] / n
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
fn Mean[T: integer | float](x: []T): f64 {
	if len(x) == 0 {
		ret 0
	}
	ret f64(Sum(x...)) / f64(len(x))
}

// Computes the weighted mean of the data set x.
//
// Mathematically, the weighted mean is defined as:
//	Mean = Σᵢ [ wᵢ * xᵢ ] / Σᵢ [ wᵢ ]
//
// If len(weights) == 0, then all of the weights are 1.
// Otherwise len(weights) must be equal to len(x).
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
#disable boundary
fn MeanWeight[T: integer | float](x: []T, weights: []T): f64 {
	if len(weights) == 0 {
		ret Mean(x)
	}
	if len(x) != len(weights) {
		panic("julenum: MeanWeight: len(x) != len(weights)")
	}
	let mut sumX: f64
	let mut sumW: f64
	mut i := 0
	for i < len(x); i++ {
		w := f64(weights[i])
		sumX += w * f64(x[i])
		sumW += w
	}
	ret sumX / sumW
}

// Computes the geometric mean of the data set x.
//
// Mathematically, the geometric mean is defined as:
//	G = (x₁ * x₂ * ... * xₙ)^(1/n)
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
fn GeometricMean[T: integer | float](x: []T): f64 {
	if len(x) == 0 {
		ret 0
	}
	mut logSum := 0.0
	for _, x_i in x {
		logSum += math::Log(f64(x_i))
	}
	logSum /= f64(len(x))
	ret math::Exp(logSum)
}

// Computes the weighted geometric mean of the data set x.
//
// Mathematically, the weighted geometric mean is defined as:
//	G = (x₁^w₁ * x₂^w₂ * ... * xₙ^wₙ)^(1 / Σᵢ [ wᵢ ])
//
// If len(weights) == 0, then all of the weights are 1.
// Otherwise len(weights) must be equal to len(x).
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
#disable boundary
fn GeometricMeanWeight[T: integer | float](x: []T, weights: []T): f64 {
	if len(weights) == 0 {
		ret GeometricMean(x)
	}
	if len(x) != len(weights) {
		panic("julenum: GeometricMeanWeight: len(x) != len(weights)")
	}
	mut logSum, mut weightsSum := 0.0, 0.0
	mut i := 0
	for i < len(x); i++ {
		w := f64(weights[i])
		logSum += w * math::Log(x[i])
		weightsSum += w
	}
	logSum /= weightsSum
	ret math::Exp(logSum)
}

// Computes the harmonic mean of the data set x.
//
// Mathematically, the harmonic mean is defined as:
//	H = n / Σᵢ [ 1 / xᵢ ]
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
#disable boundary
fn HarmonicMean[T: integer | float](x: []T): f64 {
	if len(x) == 0 {
		ret 0
	}
	mut z := 0.0
	mut i := 0
	for i < len(x); i++ {
		z += 1 / f64(x[i])
	}
	ret f64(len(x)) / z
}

// Computes the weighted harmonic mean of the data set x.
//
// Mathematically, the weighted harmonic mean is defined as:
//	H = Σᵢ [ wᵢ ] / Σᵢ [ wᵢ/xᵢ ]
//
// If len(weights) == 0, then all of the weights are 1.
// Otherwise len(weights) must be equal to len(x).
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
#disable boundary
fn HarmonicMeanWeight[T: integer | float](x: []T, weights: []T): f64 {
	if len(weights) == 0 {
		ret HarmonicMean(x)
	}
	if len(x) != len(weights) {
		panic("julenum: HarmonicMeanWeight: len(x) != len(weights)")
	}
	mut weightedSum, mut weightsSum := 0.0, 0.0
	mut i := 0
	for i < len(x); i++ {
		w := f64(weights[i])
		weightedSum += w / f64(x[i])
		weightsSum += w
	}
	ret weightsSum / weightedSum
}

// Computes the root mean square of the data set x.
//
// Mathematically, the root mean square is defined as:
//	RMS = √(Σᵢ [ xᵢ² ] / n)
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
#disable boundary
fn RootMeanSquare[T: integer | float](x: []T): f64 {
	if len(x) == 0 {
		ret 0
	}
	let mut z: f64
	mut i := 0
	for i < len(x); i++ {
		z += f64(x[i]) * f64(x[i])
	}
	z /= f64(len(x))
	ret math::Sqrt(z)
}

// Computes the weighted root mean square of the data set x.
//
// Mathematically, the weighted root mean square is defined as:
//	RMS = √(Σᵢ [ wᵢ * xᵢ² ] / Σᵢ [ wᵢ ])
//
// If len(weights) == 0, then all of the weights are 1.
// Otherwise len(weights) must be equal to len(x).
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
#disable boundary
fn RootMeanSquareWeight[T: integer | float](x: []T, weights: []T): f64 {
	if len(weights) == 0 {
		ret RootMeanSquare(x)
	}
	if len(x) != len(weights) {
		panic("julenum: RootMeanSquareWeight: len(x) != len(weights)")
	}
	mut weightedSum, mut weightsSum := 0.0, 0.0
	mut i := 0
	for i < len(x); i++ {
		w_i, x_i := f64(weights[i]), f64(x[i])
		weightedSum += w_i * x_i * x_i
		weightsSum += w_i
	}
	ret math::Sqrt(weightedSum / weightsSum)
}

// Computes the circular mean of the data set x.
//
// Mathematically, the circular mean is defined as:
//	C = atan2(Σᵢ [ sin(xᵢ) ], Σᵢ [ cos(xᵢ) ])
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
#disable boundary
fn CircularMean[T: integer | float](x: []T): f64 {
	if len(x) == 0 {
		ret 0
	}
	mut sumSin, mut sumCos := 0.0, 0.0
	mut i := 0
	for i < len(x); i++ {
		x_i := f64(x[i])
		sumSin += math::Sin(x_i)
		sumCos += math::Cos(x_i)
	}
	ret math::Atan2(sumSin, sumCos)
}

// Computes the weighted circular mean of the data set x.
//
// Mathematically, the weighted circular mean is defined as:
//	C = atan2(Σᵢ [ wᵢ * sin(xᵢ) ], Σᵢ [ wᵢ * cos(xᵢ) ])
//
// If len(weights) == 0, then all of the weights are 1.
// Otherwise len(weights) must be equal to len(x).
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
#disable boundary
fn CircularMeanWeight[T: integer | float](x: []T, weights: []T): f64 {
	if len(weights) == 0 {
		ret CircularMean(x)
	}
	if len(x) != len(weights) {
		panic("julenum: CircularMeanWeight: len(x) != len(weights)")
	}
	mut sumSin, mut sumCos := 0.0, 0.0
	mut i := 0
	for i < len(x); i++ {
		w_i, x_i := f64(weights[i]), f64(x[i])
		sumSin += w_i * math::Sin(x_i)
		sumCos += w_i * math::Cos(x_i)
	}
	ret math::Atan2(sumSin, sumCos)
}

fn medianInPlace[T: integer | float](mut x: []T): f64 {
	n := len(x)
	mid := n >> 1
	if n&1 == 1 {
		ret quickSelect(x, mid)
	}
	// For even n, average two middle values.
	lo := quickSelect(x, mid-1)
	hi := quickSelect(x, mid)
	ret (lo + hi) / 2
}

// Computes the median of the data set x.
// This function allocates a new copy of x to avoid modifying the original data.
// If preserving the original data is not necessary or x won't be used afterward,
// consider using MedianInPlace for better performance and zero allocation.
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
#disable boundary
fn Median[T: integer | float](x: []T): f64 {
	if len(x) == 0 {
		panic("julenum: Median: median of empty slice")
	}
	mut xc := make([]T, len(x))
	copy(xc, x)
	ret medianInPlace(xc)
}

// Computes the median of the data set x in-place (modifies x).
// This function mutates the input slice, and its final ordering is undefined.
// Designed for zero-allocation, memory-efficient use cases.
// If the original data must be preserved or x will be used afterward, use Median instead.
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
fn MedianInPlace[T: integer | float](mut x: []T): f64 {
	if len(x) == 0 {
		panic("julenum: MedianInPlace: median of empty slice")
	}
	medianInPlace(x)
}

// This function computes the Pearson correlation coefficient using
// a numerically stable two-pass algorithm as recommended by:
// Chan, Tony F., Gene H. Golub, and Randall J. LeVeque,
// “Algorithms for computing the sample variance: Analysis and recommendations”
//
// The two-pass method first computes the mean, then computes
// the variance and covariance using the means, reducing floating-point errors.
//
// The covariance results are unnormalised and may be used to compute biased and unbiased covariance;
//	Biased covariance:   covXY / sumW
//	Unbiased covariance: covXY / (sumW - sumW2/sumW)
fn meanCovariance[T: integer | float](x: []T, y: []T, weights: []T): (sumW: f64, sumW2: f64, meanX: f64, meanY: f64, varX: f64, varY: f64, covXY: f64) {
	mut compensationX, mut compensationY := 0.0, 0.0
	if len(weights) == 0 {
		sumW = f64(len(x))
		sumW = sumW

		// Manually inlined version of the Mean function for x and y.
		mut i := 0
		for i < len(x); i++ {
			meanX += f64(x[i])
			meanY += f64(y[i])
		}
		meanX /= f64(len(x))
		meanY /= f64(len(y))

		i = 0
		for i < len(x); i++ {
			dx := f64(x[i]) - meanX
			dy := f64(y[i]) - meanY
			varX += dx * dx
			varY += dy * dy
			covXY += dx * dy
			compensationX += dx
			compensationY += dy
		}
	} else {
		// Manually inlined version of the MeanWeight function for x and y.
		mut i := 0
		for i < len(x); i++ {
			wi := f64(weights[i])
			meanX += wi * f64(x[i])
			meanY += wi * f64(y[i])
			sumW += wi
			sumW2 += wi * wi
		}
		meanX /= sumW
		meanY /= sumW

		i = 0
		for i < len(x); i++ {
			dx := f64(x[i]) - meanX
			dy := f64(y[i]) - meanY
			wi := f64(weights[i])
			widx := wi * dx
			widy := wi * dy
			varX += widx * dx
			varY += widy * dy
			covXY += widx * dy
			compensationX += widx
			compensationY += widy
		}
	}

	varX -= compensationX * compensationX / sumW
	varY -= compensationY * compensationY / sumW
	covXY -= compensationX * compensationY / sumW

	ret
}

// Computes the Pearson correlation coefficient between two data sets x and y.
// Length of data sets must be equal. Returns 0 if lenghts are zero.
//
// Mathematically, the Pearson correlation coefficient is defined as:
//	r = Σᵢ [ (xᵢ - x̄)(yᵢ - ȳ) ] / √(Σᵢ [ (xᵢ - x̄)² ]) * √(Σᵢ [ (yᵢ - ȳ)² ])
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
#disable boundary
fn Correlation[T: integer | float](x: []T, y: []T): f64 {
	if len(x) != len(y) {
		panic("julenum: Correlation: len(x) != len(y)")
	}
	if len(x) == 0 {
		ret 0
	}
	_, _, _, _, varX, varY, covXY := meanCovariance(x, y, nil)
	denominator := math::Sqrt(varX * varY)
	if denominator == 0 {
		ret 0
	}
	ret covXY / denominator
}

// Computes the weighted Pearson correlation coefficient between two data sets x and y.
// Length of data sets must be equal. Returns 0 if lenghts are zero.
//
// Mathematically, the weighted Pearson correlation coefficient is defined as:
//	r_w = Σᵢ [ wᵢ * (xᵢ - x̄_w)(yᵢ - ȳ_w) ] / √(Σᵢ [ wᵢ * (xᵢ - x̄_w)² ]) * √(Σᵢ [ wᵢ * (yᵢ - ȳ_w)² ])
//
// If len(weights) == 0, then all of the weights are 1.
// Otherwise len(weights) must be equal to the data sets lengths.
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
#disable boundary
fn CorrelationWeight[T: integer | float](x: []T, y: []T, weights: []T): f64 {
	if len(weights) == 0 {
		ret Correlation(x, y)
	}
	if len(x) != len(y) || len(x) != len(weights) {
		panic("julenum: CorrelationWeight: len(x) != len(y) != len(weights)")
	}
	if len(x) == 0 {
		ret 0
	}
	_, _, _, _, varX, varY, covXY := meanCovariance(x, y, weights)
	denominator := math::Sqrt(varX * varY)
	if denominator == 0 {
		ret 0
	}
	ret covXY / denominator
}

// This function computes the variance using
// a numerically stable two-pass algorithm as recommended by:
// Chan, Tony F., Gene H. Golub, and Randall J. LeVeque,
// “Algorithms for computing the sample variance: Analysis and recommendations”
//
// The two-pass method first computes the mean, then computes
// the variance using the mean, reducing floating-point errors.
fn meanVariance[T: integer | float](x: []T, weights: []T): (meanX: f64, varX: f64) {
	mut n := f64(len(x))
	mut compensationX := 0.0
	if len(weights) == 0 {
		// Manually inlined version of the Mean function for x.
		mut i := 0
		for i < len(x); i++ {
			meanX += f64(x[i])
		}
		meanX /= n

		i = 0
		for i < len(x); i++ {
			dx := f64(x[i]) - meanX
			varX += dx * dx
			compensationX += dx
		}
	} else {
		// Manually inlined version of the MeanWeight function for x.
		mut sumW := 0.0
		mut i := 0
		for i < len(x); i++ {
			wi := f64(weights[i])
			meanX += wi * f64(x[i])
			sumW += wi
		}
		n = sumW
		meanX /= n

		i = 0
		for i < len(x); i++ {
			wi := f64(weights[i])
			dx := f64(x[i]) - meanX
			widx := wi * dx
			varX += widx * dx
			compensationX += widx
		}
	}

	varX -= compensationX * compensationX / n
	varX /= n - 1
	ret
}

// Computes the mean and the unbiased sample variance of the data set x.
// This is faster than calling Mean and Variance functions separately.
// Unlike Mean, it returns mean in 64-bit floating-point precision.
//
// Mathematically, the mean is defined as:
//	Mean = Σᵢ [ xᵢ ] / n
//
// Mathematically, the unbiased sample variance is defined as:
//	var(x) = Σᵢ [ (xᵢ - x̄)² ] / (n - 1)
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
fn MeanVariance[T: integer | float](x: []T): (f64, f64) {
	ret meanVariance(x, nil)
}

// Computes the unbiased sample variance of the data set x.
//
// Mathematically, the unbiased sample variance is defined as:
//	var(x) = Σᵢ [ (xᵢ - x̄)² ] / (n - 1)
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
fn Variance[T: integer | float](x: []T): f64 {
	_, varX := meanVariance(x, nil)
	ret varX
}

// Computes the unbiased sample covariance of the data set x and y.
// Length of data sets must be equal.
//
// Mathematically, the unbiased sample covariance is defined as:
//	cov(x, y) = Σᵢ [ (xᵢ - x̄)(yᵢ - ȳ) ] / (n - 1)
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
fn Covariance[T: integer | float](x: []T, y: []T): f64 {
	if len(x) != len(y) {
		panic("julenum: Covariance: len(x) != len(y)")
	}
	sumW, _, _, _, _, _, covXY := meanCovariance(x, y, nil)
	ret covXY / (sumW - 1)
}

// Computes the weighted mean and the weighted unbiased sample variance of the data set x.
// This is faster than calling MeanWeight and VarianceWeight functions separately.
// Unlike MeanWeight, it returns mean in 64-bit floating-point precision.
//
// Mathematically, the weighted mean is defined as:
//	Mean = Σᵢ [ wᵢ * xᵢ ] / Σᵢ [ wᵢ ]
//
// Mathematically, the weighted unbiased sample variance is defined as:
//	var(x) = Σᵢ [ wᵢ * (xᵢ - x̄)² ] / (Σᵢ [ wᵢ ] - 1)
//
// If len(weights) == 0, then all of the weights are 1.
// Otherwise len(weights) must be equal to len(x).
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
fn MeanVarianceWeight[T: integer | float](x: []T, weights: []T): (f64, f64) {
	if len(weights) != 0 && len(x) != len(weights) {
		panic("julenum: MeanVarianceWeight: len(x) != len(weights)")
	}
	ret meanVariance(x, weights)
}

// Computes the weighted unbiased sample variance of the data set x.
//
// Mathematically, the weighted unbiased sample variance is defined as:
//	var(x) = Σᵢ [ wᵢ * (xᵢ - x̄)² ] / (Σᵢ [ wᵢ ] - 1)
//
// If len(weights) == 0, then all of the weights are 1.
// Otherwise len(weights) must be equal to len(x).
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
fn VarianceWeight[T: integer | float](x: []T, weights: []T): f64 {
	if len(weights) != 0 && len(x) != len(weights) {
		panic("julenum: VarianceWeight: len(x) != len(weights)")
	}
	_, varX := meanVariance(x, weights)
	ret varX
}

// Computes the weighted unbiased sample covariance of the data sets x and y.
// Length of data sets must be equal.
//
// Mathematically, the weighted unbiased sample covariance is defined as:
//	cov_w(x, y) = Σᵢ [ wᵢ * (xᵢ - x̄)(yᵢ - ȳ) ] / (Σᵢ [ wᵢ ] - (Σᵢ [ wᵢ² ] / Σᵢ [ wᵢ ]))
//
// If len(weights) == 0, then all of the weights are 1.
// Otherwise len(weights) must be equal to the data sets lengths.
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
fn CovarianceWeight[T: integer | float](x: []T, y: []T, weights: []T): f64 {
	if len(weights) == 0 {
		ret Covariance(x, y)
	}
	if len(weights) != 0 && len(x) != len(weights) {
		panic("julenum: CovarianceWeight: len(x) != len(y) != len(weights)")
	}
	if len(x) != len(y) {
		panic("julenum: CovarianceWeight: len(x) != len(y)")
	}
	sumW, sumW2, _, _, _, _, covXY := meanCovariance(x, y, weights)
	ret covXY / (sumW - sumW2/sumW)
}

// Computes the Shannon entropy of a distribution or the distance between
// two distributions using natural logarithm. Returns zero for empty slice.
//
// Mathematically, the Shannon entropy is defined as:
//	-Σᵢ [ pᵢ * logₑ(pᵢ) ]
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
fn Entropy[T: integer | float](p: []T): f64 {
	mut e := 0.0
	for _, p_i in p {
		// 0 * logₑ(0) == 0
		if p_i != 0 {
			fp_i := f64(p_i)
			e += fp_i * math::Log(fp_i)
		}
	}
	ret -e
}

// Computes the cross Shannon entropy between the two distributions specified
// in p and q using natural logarithm. Returns zero for empty slice.
// Length of p and q must be equal.
//
// Mathematically, the cross Shannon entropy is defined as:
//	-Σᵢ [ pᵢ * logₑ(qᵢ) ]
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
#disable boundary
fn CrossEntropy[T: integer | float](p: []T, q: []T): f64 {
	if len(p) != len(q) {
		panic("julenum: CrossEntropy: len(p) != len(q)")
	}
	mut e := 0.0
	mut i := 0
	for i < len(p); i++ {
		// 0 * logₑ(qᵢ) == 0
		p_i := f64(p[i])
		if p_i != 0 {
			e += p_i * math::Log(f64(q[i]))
		}
	}
	ret -e
}

// Computes the Euclidean Distance between two points p1 and p2.
// Length of p1 and p2 must be equal. Returns zero for empty input.
//
// Mathematically, the Euclidean Distance is defined as:
//	d(p1, p2) = √(Σᵢ [ (p1ᵢ - p2ᵢ)² ])
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
#disable boundary
fn EuclideanDistance[T: integer | float](p1: []T, p2: []T): f64 {
	if len(p1) != len(p2) {
		panic("julenum: EuclideanDistance: len(p1) != len(p2)")
	}
	if len(p1) == 0 {
		ret 0
	}
	mut sumSquare := 0.0
	mut i := 0
	for i < len(p1); i++ {
		diff := f64(p1[i]) - f64(p2[i])
		sumSquare += diff * diff
	}
	distance := math::Sqrt(sumSquare)
	ret distance
}

// Computes the Sigmoid function.
//
// Mathematically, the Sigmoid function is defined as:
//	1 / (1 + exp(-x))
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
fn Sigmoid[T: integer | float](x: T): f64 {
	ret 1 / (1 + math::Exp(f64(-x)))
}

// Computes the best-fit line
//
//	y = alpha + beta*x
//
// to the data in x and y. If origin is true, the
// regression is forced to pass through the origin.
// Length of x and y must be equal.
//
// Specifically, computes the values of alpha and
// beta such that the total residual
//
//	Σᵢ [ (yᵢ - alpha - beta*xᵢ)² ]
//
// is minimized. If origin is true, then alpha is forced to be zero.
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
#disable boundary
fn LinearRegression[T: integer | float](x: []T, y: []T, origin: bool): (alpha: f64, beta: f64) {
	if len(x) != len(y) {
		panic("julenum: LinearRegression: len(x) != len(y)")
	}

	if origin {
		mut x2Sum, mut xySum := 0.0, 0.0
		mut i := 0
		for i < len(x); i++ {
			xi, yi := f64(x[i]), f64(y[i])
			xySum += xi * yi
			x2Sum += xi * xi
		}
		beta = xySum / x2Sum

		ret 0, beta
	}

	_, _, meanX, meanY, varX, _, covXY := meanCovariance(x, y, nil)
	beta = covXY / varX
	alpha = meanY - beta*meanX
	ret
}

// Computes the weighted best-fit line
//
//	y = alpha + beta*x
//
// to the data in x and y. If origin is true, the
// regression is forced to pass through the origin.
// Length of x and y must be equal.
//
// Specifically, computes the values of alpha and
// beta such that the total residual
//
//	Σᵢ [ wᵢ * (yᵢ - alpha - beta*xᵢ)² ]
//
// is minimized. If origin is true, then alpha is forced to be zero.
//
// If len(weights) == 0, then all of the weights are 1.
// Otherwise len(weights) must be equal to len(x).
//
// Computations are performed using 64-bit floating-point precision and
// result is in the same precision, casting to proper type is responsibility of the developer.
#disable boundary
fn LinearRegressionWeight[T: integer | float](x: []T, y: []T, weights: []T, origin: bool): (alpha: f64, beta: f64) {
	if len(weights) == 0 {
		ret LinearRegression(x, y, origin)
	}
	if len(x) != len(y) {
		panic("julenum: LinearRegressionWeight: len(x) != len(y)")
	}
	if len(x) != len(weights) {
		panic("julenum: LinearRegressionWeight: len(x) != len(y) != len(weights)")
	}

	if origin {
		mut x2Sum, mut xySum := 0.0, 0.0
		mut i := 0
		for i < len(x); i++ {
			w, xi, yi := f64(weights[i]), f64(x[i]), f64(y[i])
			xySum += w * xi * yi
			x2Sum += w * xi * xi
		}
		beta = xySum / x2Sum

		ret 0, beta
	}

	_, _, meanX, meanY, varX, _, covXY := meanCovariance(x, y, weights)
	beta = covXY / varX
	alpha = meanY - beta*meanX
	ret
}

// Computes the standard score (a.k.a. z-score, z-value) for the value x
// with the given mean and standard deviation, i.e.
//
// Mathematically, the standard score is defined as:
//	(x - mean) / stdDev
fn StdScore(x: f64, mean: f64, stdDev: f64): f64 {
	ret (x - mean) / stdDev
}