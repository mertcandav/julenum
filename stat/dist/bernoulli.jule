// Copyright 2025 mertcandav.
// Use of this source code is governed by a BSD 3-Clause
// license that can be found in the LICENSE file.

use "std/math"
use "std/math/rand"

// Represents an unvariate Bernoulli distribution.
// It models a single experiment with two possible outcomes: success (1) or failure (0).
// The parameter 'P' is the probability of success.
//
// Uses the RNG for randomness. It may be nil.
// Implementation will use global random functions if RNG is nil.
//
// Parameters:
//	P: Probability of success (0 <= p <= 1).
struct Bernoulli {
	P:   f64
	RNG: &rand::Rand
}

impl Bernoulli {
	// Returns the number of parameters required to define the distribution.
	fn NumParams(*self): int {
		ret 1
	}

	// Computes the Cumulative Distribution Function (CDF) at x.
	// The CDF is the probability that a random variable X is less than or equal to a given value x.
	//
	// For a distribution:
	//	- If x < 0: CDF(x) = 0
	//	- If 0 <= x < 1: CDF(x) = 1 - p (Probability of failure)
	//	- If x >= 1: CDF(x) = 1 (Probability of success or failure, always 1)
	fn CDF(*self, x: f64): f64 {
		if x < 0 {
			ret 0
		} else if x >= 0 && x < 1 {
			ret 1 - self.P // P(X <= 0) = P(X=0)
		}
		ret 1 // P(X <= 1) = P(X=0) + P(X=1) = 1-p + p = 1
	}

	// Computes the Probability Mass Function (PMF) at x.
	// The PMF gives the probability that a discrete random variable is exactly equal to some value.
	//
	// For a distribution:
	//	- P(X=1) = p
	//	- P(X=0) = 1 - p
	//	- P(X=x) = 0 for x != 0 and x != 1
	fn PMF(*self, x: f64): f64 {
		if x == 1 {
			ret self.P
		} else if x == 0 {
			ret 1 - self.P
		}
		ret 0 // Probability is 0 for any other value of x
	}

	// Computes the natural logarithm of the Probability Mass Function (PMF) at x.
	// This is often used in maximum likelihood estimation to avoid underflow with very small probabilities.
	//
	// For a distribution:
	//	- If x = 1: log(P(X=1)) = log(p)
	//	- If x = 0: log(P(X=0)) = log(1-p)
	//	- Otherwise: log(0) = -Inf
	fn LogPMF(*self, x: f64): f64 {
		if x == 1 {
			// If P is 0, probability of 1 is 0, so log(0) = -Inf
			ret math::Log(self.P)
		}
		if x == 0 {
			// If P is 1, probability of 0 is 0, so log(0) = -Inf
			ret math::Log(1 - self.P)
		}
		ret math::Inf(-1) // Probability is 0 for any other value of x
	}

	// Computes the Moment Generating Function (MGF) of the distribution at t.
	// The MGF is useful for deriving moments of the distribution.
	//
	// Formula:
	//	M_X(t) = E[e^(tX)] = (1 - p) + p * e^t
	fn MGF(*self, t: f64): f64 {
		ret (1 - self.P) + self.P*math::Exp(t)
	}

	// Computes the Survival Function (SF) at x, also known as the Complementary Cumulative Distribution Function (CCDF).
	// The Survival Function is the probability that a random variable X is greater than a given value x.
	// SF(x) = P(X > x) = 1 - CDF(x)
	fn Survival(*self, x: f64): f64 {
		ret 1 - self.CDF(x)
	}

	// Computes the Quantile Function (inverse CDF) for the distribution.
	// The quantile function returns the value x such that the probability of a random variable
	// being less than or equal to x is p. Returns NaN if p is outside [0, 1].
	fn Quantile(*self, p: f64): f64 {
		if p < 0 || p > 1 {
			ret math::NaN()
		}
		if p <= (1-self.P) {
			ret 0
		}
		ret 1
	}

	// Computes the entropy of the distribution.
	// Entropy is a measure of the uncertainty or randomness of the distribution.
	//
	// Formula:
	//	H(X) = -p * log2(p) - (1-p) * log2(1-p)
	//	Note: If p is 0 or 1, the corresponding term (e.g., 0 * log(0)) is taken as 0.
	fn Entropy(*self): f64 {
		if self.P == 0 || self.P == 1 {
			ret 0 // Entropy is 0 for deterministic outcomes
		}
		mut term1 := 0.0
		if self.P > 0 {
			term1 = -self.P * math::Log2(self.P)
		}
		mut term2 := 0.0
		q := 1 - self.P
		if q > 0 {
			term2 = -q * math::Log2(q)
		}
		ret term1 + term2
	}

	// Computes the excess kurtosis of the distribution.
	// Excess kurtosis measures the "tailedness" of the distribution relative to a normal distribution.
	// A positive excess kurtosis indicates "fat tails" (more outliers), while a negative value indicates "thin tails".
	//
	// Formula:
	//	(1 - 6p(1-p)) / (p(1-p))
	fn ExcessKurtosis(*self): f64 {
		if self.P == 0 || self.P == 1 {
			ret math::Inf(1) // Kurtosis is infinite for degenerate distributions
		}
		pq := self.P * (1 - self.P)
		ret (1 - 6*pq) / pq
	}

	// Computes the mean (expected value) of the distribution.
	//
	// Formula:
	//	E[X] = p
	fn Mean(*self): f64 {
		ret self.P
	}

	// Computes the median of the distribution.
	// The median is the value separating the higher half from the lower half of a data sample.
	//
	// For a distribution:
	//	- If p < 0.5: Median = 0
	//	- If p > 0.5: Median = 1
	//	- If p = 0.5: Median can be any value between 0 and 1 (inclusive),
	//	  but commonly defined as 0.5 or either 0 or 1. Here, we return 0.5 for consistency.
	fn Median(*self): f64 {
		if self.P < 0.5 {
			ret 0
		} else if self.P > 0.5 {
			ret 1
		}
		ret 0.5 // For p = 0.5, any value in [0, 1] is a median. 0.5 is a common choice.
	}

	// Computes the mode (most likely value) of the distribution.
	// For a distribution, the mode is:
	//	- 0 if p < 0.5
	//	- 1 if p > 0.5
	//	- Either 0 or 1 if p = 0.5. Here, we return 0 for consistency.
	fn Mode(*self): f64 {
		if self.P < 0.5 {
			ret 0
		} else if self.P > 0.5 {
			ret 1
		}
		ret 0 // p = 0.5: both 0 and 1 are modes. Choose 0.
	}

	// Returns the support (domain) of the distribution.
	// The support is the set of possible values the random variable can take.
	fn Support(*self): (min: f64, max: f64) {
		ret 0, 1
	}

	// Returns a random number from the distribution.
	// It returns 1 with probability 'P' and 0 with probability '1-p'.
	fn Rand(*self): f64 {
		if self.RNG == nil {
			if rand::F64() < self.P {
				ret 1
			}
		} else {
			if self.RNG.F64() < self.P {
				ret 1
			}
		}
		ret 0
	}

	// Computes the skewness of the distribution.
	// Skewness is a measure of the asymmetry of the probability distribution about its mean.
	// A positive skewness indicates a tail on the right side, and negative indicates a tail on the left.
	//
	// Formula:
	//	Skewness = (1 - 2p) / sqrt(p(1-p))
	//	Note: Returns NaN if p is 0 or 1.
	fn Skewness(*self): f64 {
		if self.P == 0 || self.P == 1 {
			ret math::NaN() // Skewness is undefined for degenerate distributions
		}
		ret (1 - 2*self.P) / math::Sqrt(self.P*(1-self.P))
	}

	// Computes the variance of the distribution.
	// Variance measures how far the numbers are spread out from the average value.
	//
	// Formula:
	//	Var(X) = p * (1 - p)
	fn Variance(*self): f64 {
		ret self.P * (1 - self.P)
	}

	// Computes the standard deviation of the distribution.
	// The standard deviation is the square root of the variance, measuring the spread of the distribution.
	//
	// Formula:
	//	StdDev = sqrt(p * (1 - p))
	fn StdDev(*self): f64 {
		ret math::Sqrt(self.Variance())
	}

	// Estimates the distribution parameter (P) from a sample of observations.
	//
	// This method performs Maximum Likelihood Estimation (MLE) using the formula:
	//	p̂ = (1 / n) * Σᵢ [ xᵢ ]
	//
	// All sample values must be either 0 or 1.
	// If the sample contains any value outside this set, it panics.
	fn Fit(mut *self, samples: []f64) {
		n := len(samples)
		if n == 0 {
			panic("dist: cannot fit Bernoulli distribution to empty sample")
		}
		let mut sum: f64
		for _, x in samples {
			if x != 0 && x != 1 {
				panic("dist: invalid sample value, expected 0 or 1")
			}
			sum += x
		}
		self.P = sum / f64(n)
	}
}